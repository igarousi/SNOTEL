{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the Observed Snow Data using CUAHSI Data Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This noteook retrieves Snow Water Equivalent (SWE) and accumulated precipitation (P) data from SNOTEL sites through CUAHSI data client service.  The notebook has five sections as following:\n",
    "\n",
    "1. Import Libraries\n",
    "2. Visualize Ecoregions\n",
    "3. Define Parameters\n",
    "4. Define Functions \n",
    "5. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "In this section, required python libraries are installed and imported. Use `snotel` kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ulmo  \n",
    "import pytz\n",
    "import datetime\n",
    "from timezonefinder import TimezoneFinder\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "import glob\n",
    "from matplotlib import colors\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from suds.client import Client\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Grouper  \n",
    "from datetime import timedelta\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.   Read SNOTEL Information\n",
    "Information includes but not limited to station code, station latitude and longitude, and associated ecoregions provided by the Commission for Environmental Corporation (CEC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel_network = '../input/NRCS_SNOTEL_Joint_w_CEC.csv'  \n",
    "output_dir = '../output'\n",
    "snotel_info = pd.read_csv(os.path.join(os.getcwd(), snotel_network))\n",
    "snotel_info = snotel_info[snotel_info['Join_Count'] == 1]                 # To make sure only those associated with CEC are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.   Define Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following parameters identify the name of the network from which we want to retrieve data. It also creates the CUAHSI link to the defined network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start preparing input arguments that are required when connecting to CUAHSI data client service\n",
    "network = \"SNOTEL\"     \n",
    "wsdl = f'http://hydroportal.cuahsi.org/{network.lower()}/cuahsi_1_1.asmx?WSDL'\n",
    "client = Client(wsdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.   Define Functions\n",
    "Two functions are defined in this Jupyter Notebook. *DataFrame* and *Local2UTC*.  The first function is used to retrieve SNOTEL data.  The second function is used when we want to transform local time to UTC time.  \n",
    "\n",
    "- **DataFrame**: This function creates a dataframe for retrieved data including retrieved information. \n",
    "\n",
    "\n",
    "- **Local2UTC**: There are two different time zones within these ecoregions, Mountain Time and Pacific Time zones.  'America/Phoenix', is in the Mountain Time zone and does not observe daylight saving time.  It always has 7 hours offset from UTC.  'America/Boise' and 'America/Denver' are also in the Mountain Time zone with 6 hours offset from UTC during daylight saving time and 7 hours offset in standard time.  'America/Los_Angeles' is in the Pacific Time with 7 hours offset from UTC during daylight saving time and 8 hours ofset in standard time.  These are important when comparing daily average values of SNOTEL with hourly datasets such as NWM where outputs are in UTC time or SNODAS snapshots that are reported at 6:00 UTC.  The Local2UTC function takes a dataframe that has a column called 'date' (showing the local time for each record (row)) and a list of integer indices of the dataframe as inputs.  The function runs over each row of the dataframe and reads corresponding latitude and longitude values to get the name of the time zone ('America/Phoenix', America/Boise', 'America/Denver', or 'America/Los_Angeles').  Then, it uses pytz library and and transform the local date/time to a UTC date/time.  The next part checks for the offset value and updates the time for the local time that by default is 00:00:00.  This update helps to choose a time for SNOTEL local time that is equivalent to 6:00 UTC (as used in SNODAS outputs).\n",
    "\n",
    "\n",
    "- **Data_Retrieval**: This function uses snotel information (csv file), DataFrame, and Local2UTC functions to retrieve SNOTEL data from CUAHSI data client service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to make a dataframe from retrieved data.  \n",
    "# This function can be used for both SWE and Precipitation retrievals.\n",
    "\n",
    "def DataFrame(self, st, et):\n",
    "    \n",
    "    # series info\n",
    "    qo = self.queryInfo\n",
    "    self.site_code = qo.criteria.locationParam\n",
    "    self.variable_code = qo.criteria.variableParam\n",
    "    self.start = st \n",
    "    self.end = et \n",
    "\n",
    "    # source info\n",
    "    si = self.timeSeries[0].sourceInfo\n",
    "    self.site_name = si.siteName\n",
    "    self.latitude = si.geoLocation.geogLocation.latitude\n",
    "    self.longitude = si.geoLocation.geogLocation.longitude\n",
    "    \n",
    "    # variable\n",
    "    v = self.timeSeries[0].variable\n",
    "    self.variable_name = v.variableName\n",
    "    self.variable_datatype = v.dataType\n",
    "    self.units_abbv = v.unit.unitAbbreviation\n",
    "    self.nodata = v.noDataValue\n",
    "\n",
    "    # values\n",
    "    self.data = []\n",
    "    for val in self.timeSeries[0].values[0].value:\n",
    "        value_dt  = val._dateTime\n",
    "        value_in = float(val.value)\n",
    "        if value_in != self.nodata:\n",
    "            value_mm = value_in * 25.4\n",
    "        else:\n",
    "            value_in = np.NaN\n",
    "            value_mm = np.NaN\n",
    "        self.data.append(dict(date=value_dt,\n",
    "                              value_inches=value_in,\n",
    "                              value_mm=value_mm))\n",
    "     \n",
    "    atts = {k:v for k,v in self.__dict__.items()}\n",
    "\n",
    "    dat = []\n",
    "    for val in self.data:\n",
    "        content = {k:v for k,v in atts.items()}\n",
    "        for k, v in val.items():\n",
    "            content[k] = v\n",
    "        dat.append(content)  \n",
    "\n",
    "    df = pd.DataFrame(dat)\n",
    "    df = df.set_index(df.date)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that transform local time to UTC.\n",
    "\n",
    "def Local2UTC(data, indices):\n",
    "    \n",
    "    tf = TimezoneFinder()\n",
    "\n",
    "    LOCAL = []\n",
    "    UTC = []\n",
    "    \n",
    "    # Loop over each row of the dataframe    \n",
    "    for row in indices:\n",
    "                \n",
    "        timezone_str = tf.timezone_at(lng=data.longitude[row], lat=data.latitude[row])\n",
    "        local_time = pytz.timezone(timezone_str)\n",
    "        # The next line uses the date and by default gives the time 00:00:00 \n",
    "        # (i.e., 12:00 am) to the datetime object\n",
    "        naive_datetime = datetime.strptime(data['date'][row].strftime('%Y-%m-%d %H:%M:%S'), \"%Y-%m-%d %H:%M:%S\") \n",
    "        local_datetime = local_time.localize(naive_datetime, is_dst=None)\n",
    "        utc_datetime = local_datetime.astimezone(pytz.utc)\n",
    "        \n",
    "        # Update naive_time based on timezone. This update helps to choose a \n",
    "        # time for SNOTEL daily values that are equivalent to 6:00 am UTC\n",
    "        \n",
    "        # Condition 1: if (local_time == \"America/Phoenix\") or \n",
    "        # (local_time == \"America/Denver\" and utc_datetime.hour == 7, Standard) or\n",
    "        # (local_time == \"America/Boise\" and utc_datetime.hour == 7, Standard) or\n",
    "        # (local_time == \"America/Los_Angeles\" and utc_datetime.hour == 7, Daylight)\n",
    "        if utc_datetime.hour == 7:\n",
    "            naive_datetime = naive_datetime + timedelta(hours=23)\n",
    "            local_datetime = local_time.localize(naive_datetime, is_dst=None)\n",
    "            utc_datetime = local_datetime.astimezone(pytz.utc)\n",
    "            utc_datetime_str = utc_datetime.strftime (\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "        # Condition 2: if (local_time == \"America/Denver\" and utc_datetime.hour == 6, Daylight) or\n",
    "        # (local_time == \"America/Boise\" and utc_datetime.hour == 6, Daylight)\n",
    "        elif utc_datetime.hour == 6: \n",
    "            naive_datetime = naive_datetime\n",
    "            local_datetime = local_time.localize(naive_datetime, is_dst=None)\n",
    "            utc_datetime = local_datetime.astimezone(pytz.utc)\n",
    "            utc_datetime_str = utc_datetime.strftime (\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "        # Condition 3: if (local_time == \"America/Los_Angeles\" and utc_datetime.hour == 8, Standard)\n",
    "        elif utc_datetime.hour == 8: \n",
    "            naive_datetime = naive_datetime + timedelta(hours=22)\n",
    "            local_datetime = local_time.localize(naive_datetime, is_dst=None)\n",
    "            utc_datetime = local_datetime.astimezone(pytz.utc)\n",
    "            utc_datetime_str = utc_datetime.strftime (\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "        \n",
    "        LOCAL.append(local_datetime)\n",
    "        UTC.append(utc_datetime_str)\n",
    "        \n",
    "        \n",
    "    # Add columns to the dataframe\n",
    "    data['datetime_LOCAL'] = LOCAL\n",
    "    data['datetime_UTC'] = UTC\n",
    "        \n",
    "    # Add UTC times to index\n",
    "    data.index = data['datetime_UTC']\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that retrieves data from CUAHSI data services.\n",
    "\n",
    "def Data_Retrieval(info_dataframe, cec_name, variable, abbr, output_dir, output_name):\n",
    "    \n",
    "    # Select data for a region and print the number of snotel gages within this domain\n",
    "    snotel_region = info_dataframe[info_dataframe['NAME'] == cec_name]\n",
    "    snotel_region.reset_index(drop=True, inplace=True)   # if not, you will get error in site_code[i] or info_dataframe[i] later!\n",
    "    print(\"There are \", snotel_region.shape[0], \" SNOTEL gages within \", cec_name)\n",
    "    \n",
    "    \n",
    "    # Retieve data\n",
    "    site_code = snotel_region['Station_ID']\n",
    "    variable_code_daily = f'{network}:{variable}'\n",
    "    var_Daily = pd.DataFrame([])\n",
    "    \n",
    "    for i in range(0, len(site_code)):\n",
    "        \n",
    "        code = f'{site_code[i]}_{snotel_region[\"State\"][i]}_SNTL'\n",
    "        sitecode = f'{network}:{code}'\n",
    "        site = ulmo.cuahsi.wof.get_site_info(wsdl, sitecode, suds_cache=('default', ))    \n",
    "        sc = f'{network}:{site[\"code\"]}'   \n",
    "        st = datetime.strptime(site['site_property']['site_comments'].split('|')[0].split('=')[1].split(' ')[0], '%m/%d/%Y')  \n",
    "        et = datetime.strptime(site['site_property']['site_comments'].split('|')[1].split('=')[1].split(' ')[0], '%m/%d/%Y')\n",
    "        \n",
    "        try:\n",
    "            temp = client.service.GetValuesObject(sc, variable_code_daily, st, et, '')\n",
    "            # Use DataFrame function defined above\n",
    "            temp_df = DataFrame(temp, st, et)\n",
    "            var_Daily = var_Daily.append(temp_df, ignore_index=True)\n",
    "            print(i, sc, st, et)  \n",
    "        except Exception as error:\n",
    "            print(\"==========================================================\")\n",
    "            print(f'{variable} at {sc} with index {i} in site_code is not available.')\n",
    "            print(\"==========================================================\")\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    Daily_small_df = pd.DataFrame({'col1':var_Daily['site_code'], \n",
    "                                   'col2':var_Daily['site_name'], \n",
    "                                   'col3':var_Daily['date'], \n",
    "                                   'col4':var_Daily['value_inches'],\n",
    "                                   'col5':var_Daily['value_mm'],\n",
    "                                   'col6':var_Daily['latitude'],\n",
    "                                   'col7':var_Daily['longitude']}) \n",
    "              \n",
    "    \n",
    "    Daily_small_df.columns = ['site_code', 'site_name', 'date', f'{abbr}_inches', \n",
    "                               f'{abbr}_mm', 'latitude', 'longitude']\n",
    "\n",
    "    \n",
    "    # Reset indices to make sure there is not gap and then create a column called 'name' that includes ecoregion names\n",
    "    snotel_region.reset_index(drop=True, inplace=True)\n",
    "    for c in range (0, len(site_code)):\n",
    "        Daily_small_df.loc[Daily_small_df['site_name'] == snotel_region['Station_Na'][c], 'name'] = snotel_region['NAME'][c]\n",
    "        \n",
    "        \n",
    "    # Run Local2UTC function defined above\n",
    "    Daily_small_df_utc = Local2UTC(Daily_small_df, Daily_small_df.index)\n",
    "        \n",
    "        \n",
    "    # Save results as a CSV file\n",
    "    cec_name = cec_name.replace(\" \",\"\")\n",
    "    cec_name = cec_name.replace(\"/\",\"\")\n",
    "    Daily_small_df_utc.to_csv(f'{output_dir}/{output_name}_{cec_name}.csv', index=False)\n",
    "\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that retrieves data from CUAHSI data services.\n",
    "\n",
    "def Data_Retrieval(info_dataframe, cec_name, variable, abbr, output_dir, output_name):\n",
    "    \n",
    "    # Select data for a region and print the number of snotel gages within this domain\n",
    "    snotel_region = info_dataframe[info_dataframe['NAME'] == cec_name]\n",
    "    snotel_region.reset_index(drop=True, inplace=True)   # if not, you will get error in site_code[i] or info_dataframe[i] later!\n",
    "    print(\"There are \", snotel_region.shape[0], \" SNOTEL gages within \", cec_name)\n",
    "    \n",
    "    \n",
    "    # Create a list of sites that are retrievable. This list helps \n",
    "    # not getting error when running the next loop (i.e., retrieving data)\n",
    "    code = []\n",
    "    site_code = []\n",
    "    site = []\n",
    "    for c in range(0, snotel_region.shape[0]):\n",
    "        try:\n",
    "            cd = f'{snotel_region[\"Station_ID\"][c]}_{snotel_region[\"State\"][c]}_SNTL'\n",
    "            sc = f'{network}:{cd}'\n",
    "            st = ulmo.cuahsi.wof.get_site_info(wsdl, sc, suds_cache=('default', ))\n",
    "            code.append(cd)\n",
    "            site_code.append(sc)\n",
    "            site.append(st)\n",
    "        except Exception as error:\n",
    "            print(\"========================================\")\n",
    "            print(cd, f'with index {c} cannot be retrieved.')\n",
    "            print(\"========================================\")\n",
    "            pass  \n",
    "        \n",
    "    \n",
    "    # Retieve data\n",
    "    variable_code_daily = f'{network}:{variable}'\n",
    "    var_Daily = pd.DataFrame([])\n",
    "    for i in range(0, len(site_code)):\n",
    "        sc = f'{network}:{site[i][\"code\"]}'   \n",
    "        st = datetime.strptime(site[i]['site_property']['site_comments'].split('|')[0].split('=')[1].split(' ')[0], '%m/%d/%Y')  \n",
    "        et = datetime.strptime(site[i]['site_property']['site_comments'].split('|')[1].split('=')[1].split(' ')[0], '%m/%d/%Y')\n",
    "        \n",
    "        try:\n",
    "            temp = client.service.GetValuesObject(sc, variable_code_daily, st, et, '')\n",
    "            # Use DataFrame function defined above\n",
    "            temp_df = DataFrame(temp, st, et)\n",
    "            var_Daily = var_Daily.append(temp_df, ignore_index=True)\n",
    "            print(i, sc, st, et)  \n",
    "        except Exception as error:\n",
    "            print(\"==========================================================\")\n",
    "            print(f'{variable} at {sc} with index {i} in site_code is not available.')\n",
    "            print(\"==========================================================\")\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    Daily_small_df = pd.DataFrame({'col1':var_Daily['site_code'], \n",
    "                                   'col2':var_Daily['site_name'], \n",
    "                                   'col3':var_Daily['date'], \n",
    "                                   'col4':var_Daily['value_inches'],\n",
    "                                   'col5':var_Daily['value_mm'],\n",
    "                                   'col6':var_Daily['latitude'],\n",
    "                                   'col7':var_Daily['longitude']}) \n",
    "              \n",
    "    \n",
    "    Daily_small_df.columns = ['site_code', 'site_name', 'date', f'{abbr}_inches', \n",
    "                               f'{abbr}_mm', 'latitude', 'longitude']\n",
    "\n",
    "    \n",
    "    # Reset indices to make sure there is not gap and then create a column called 'name' that includes ecoregion names\n",
    "    snotel_region.reset_index(drop=True, inplace=True)\n",
    "    for c in range (0, len(site_code)):\n",
    "        Daily_small_df.loc[Daily_small_df['site_name'] == snotel_region['Station_Na'][c], 'name'] = snotel_region['NAME'][c]\n",
    "        \n",
    "        \n",
    "    # Run Local2UTC function defined above\n",
    "    Daily_small_df_utc = Local2UTC(Daily_small_df, Daily_small_df.index)\n",
    "        \n",
    "        \n",
    "    # Save results as a CSV file\n",
    "    cec_name = cec_name.replace(\" \",\"\")\n",
    "    cec_name = cec_name.replace(\"/\",\"\")\n",
    "    Daily_small_df_utc.to_csv(f'{output_dir}/{output_name}_{cec_name}.csv', index=False)\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.   Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following retrieves all available snow water equivalent (SWE) and precipitation data for all gages. To improve the speed and prevent errors, the retrieval algorithm loops over ecoregions and then finally I will merge them all into one singe CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cec_regions = ['Canadian Rockies',\n",
    "               'Northern Basin and Range',\n",
    "               'Eastern Cascade Slopes and Foothills',\n",
    "               'Arizona/New Mexico Mountains',\n",
    "               'Southern Rockies',\n",
    "               'Idaho Batholith',\n",
    "               'Columbia Mountains/Northern Rockies',\n",
    "               'Central Basin and Range',\n",
    "               'Klamath Mountains',\n",
    "               'Cascades',\n",
    "               'Middle Rockies',\n",
    "               'Blue Mountains',\n",
    "               'North Cascades',\n",
    "               'Wasatch and Uinta Mountains',\n",
    "               'Sierra Nevada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cec_name in cec_regions:\n",
    "    Data_Retrieval(snotel_info, cec_name, 'WTEQ_D', 'swe', output_dir, 'SNOTEL_SWE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cec_name in cec_regions:\n",
    "    Data_Retrieval(snotel_info, cec_name, 'PREC_D', 'precip', output_dir, 'SNOTEL_P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Combine Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open all SWE files and save as one single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.DataFrame([])\n",
    "\n",
    "for f in glob.glob(os.path.join(output_dir, '*SWE_*.csv')):\n",
    "    data = pd.read_csv(f)\n",
    "    frames = [Data, data]\n",
    "    Data = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "Data.to_csv(os.path.join(output_dir, 'SNOTEL_SWE.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open all Precipitation files and save as one single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.DataFrame([])\n",
    "\n",
    "for f in glob.glob(os.path.join(output_dir, '*P_*.csv')):\n",
    "    data = pd.read_csv(f)\n",
    "    frames = [Data, data]\n",
    "    Data = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "Data.to_csv(os.path.join(output_dir, 'SNOTEL_P.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  Filter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells loop over all sites and for each day in the time period of interest, extract the values of snow water equivalent (LDASOUT outputs) or precipitation (FORCING iputs) as well as some other information related to the gage, and return a dataframe including 4 columns (Site_ID, Ecoregion_Name, Date_Time_UTC, and variale of interest) as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel_p = pd.read_csv(os.path.join(output_dir,'SNOTEL_P.csv'))\n",
    "snotel_swe = pd.read_csv(os.path.join(output_dir,'SNOTEL_SWE.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NWM and SNODAS dataframes have site codes as integer values, but SNOTEL has them as string. To be consistent, I will change strings into inegers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel_p['site_code'] = snotel_p['site_code'].map(lambda x : int(x.split(\":\")[1].split(\"_\")[0]))\n",
    "snotel_swe['site_code'] = snotel_swe['site_code'].map(lambda x : int(x.split(\":\")[1].split(\"_\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Define a Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_Daily = pd.date_range('2007-10-01', '2018-10-01', freq='1D')\n",
    "dates_Daily= dates_Daily + timedelta(hours=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Define Filter Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter_SNO(dataset, column_code, column_date, column_ecoregion, column_val, final_var_name, csv_name, output_dir):\n",
    "    \n",
    "    '''\n",
    "    dataset:           Dataset for which Filter function is used\n",
    "    column_code:       Column including site names in the dataframe\n",
    "    column_date:       Column including dates in the dataframe\n",
    "    column_ecoregion:  Column including ecoregion names in the dataframe\n",
    "    column_val:        Column including values of the variable of interest\n",
    "    final_column_val:  Column including values of the variable of interests as results \n",
    "    csv_name:          Name of the output (i.e., a CSV file)\n",
    "    output_dir:        Path to save outputs\n",
    "    '''\n",
    "    \n",
    "    # Get station ids\n",
    "#    code = set(dataset1[column_code])  # List of site codes \n",
    "    code = [1107, 1000, 823, 353, 669, 1127, 423, 376]\n",
    "    \n",
    "    ID = []\n",
    "    NAME = []\n",
    "    TIME = []\n",
    "    VALUE = []\n",
    "    for c in code:\n",
    "        select = dataset.loc[dataset[column_code] == c]\n",
    "        for d in dates_Daily:\n",
    "            try:\n",
    "                id = select[pd.to_datetime(select[column_date]) == d][column_code].values[0]\n",
    "                name = select[pd.to_datetime(select[column_date]) == d][column_ecoregion].values[0]\n",
    "                value = select[pd.to_datetime(select[column_date]) == d][column_val].values[0]\n",
    "            except Exception as e:\n",
    "                id = dataset[dataset[column_code] == c][column_code].values[0]\n",
    "                name = dataset[dataset[column_code] == c][column_ecoregion].values[0]\n",
    "                value = np.nan\n",
    "            ID.append(id)\n",
    "            NAME.append(name)\n",
    "            TIME.append(d)\n",
    "            VALUE.append(value)\n",
    "\n",
    "    df = pd.DataFrame({'col1': ID, 'col2': NAME, 'col3': TIME, 'col4': VALUE})\n",
    "    df.columns = ['Site_Code',  'Ecoregion_Name', 'Date_Time_UTC', final_var_name] \n",
    "    df.index = df['Date_Time_UTC']\n",
    "    df.to_csv(os.path.join(output_dir, csv_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Apply Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter_SNO(vars()['snotel_swe'], 'site_code', 'datetime_UTC', 'name', 'swe_mm', 'SWE_mm', 'SNOTEL_SWE_Filter.csv', output_dir)\n",
    "Filter_SNO(vars()['snotel_p'], 'site_code', 'datetime_UTC', 'name', 'precip_mm', 'P_mm', 'SNOTEL_P_Filter.csv', output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snotel",
   "language": "python",
   "name": "snotel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
